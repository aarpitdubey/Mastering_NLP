{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Parse HTML\n",
    "\n",
    "Use the request library to fetch the HTML content of a webpage and then use BeautifulSoup to parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://growdataskills.com/project-nlp\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text Data\n",
    "\n",
    "Once you have the parsed HTML, extract the relevant text data using various methods such as `.find()`, `.find_all()`, and `.get_text()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extracting all paragraphs\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Extracting text from each paragraph\n",
    "paragraph_texts = [paragraph.get_text() for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Text preprocessing invovles various steps to clean and normalize the extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "lowercase_text = [text.lower() for text in paragraph_texts]\n",
    "\n",
    "# Remove special characters using regex\n",
    "cleaned_text = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in lowercase_text]\n",
    "\n",
    "# Tokenization\n",
    "tokenized_text = [word_tokenize(text) for text in cleaned_text]\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered__text = [[word for word in tokens if word not in stop_words] for tokens in tokenized_text]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_text = [[stemmer.stem(word) for word in tokens] for tokens in filtered__text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Processing\n",
    "\n",
    "You can performed additional steps such as removing empty tokens, converting the processed text back to sentences or paragraphs, and so on, based on your requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty tokens\n",
    "final_text = [[word for word in tokens if word.strip()] for tokens in stemmed_text]\n",
    "\n",
    "# Convert tokens back to sentences\n",
    "sentences = [' '.join(tokens) for tokens in final_text]\n",
    "\n",
    "# Convert sentences bak to paragraphs\n",
    "processed_paragraphs = '\\n\\n'.join(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Processed Text\n",
    "\n",
    "finally, you can save the processed text to a file for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shubhankit sirvaiya data scientist world wide technolog ex accentur growth strategi mentor 700 student around data scienc domain\n",
      "\n",
      "empow aspir data profession soar upskil opportun\n",
      "\n",
      "copyright 2023 grow data skill right reserv\n",
      "\n",
      "welcom grow data skill chat support repres respond within hour\n",
      "\n",
      "enquiri\n"
     ]
    }
   ],
   "source": [
    "with open('processed_text.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(processed_paragraphs)\n",
    "    \n",
    "print(processed_paragraphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
